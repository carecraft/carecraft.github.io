---
layout:     post
title:      "认识 DPDK"
subtitle:   "understand the theory of DPDK"
category :  basictheory
date:       2017-07-01
author:     "Max"
header-img: "img/post-bg-sync.jpg"
catalog:    true
tags:
    - opensource
---

## 1. 引言

项目上选用 dpdk 收包，因此对其做个初步了解，特此记录。

[DPDK](http://www.dpdk.org/) 全称 Data Plane Development Kit，是 Intel 推出的高速数据包处理套件，依据官方性能测试报告[ DPDK 17.05 Intel NIC Performance Report ](http://fast.dpdk.org/doc/perf/DPDK_17_05_Intel_NIC_performance_report.pdf)，单核性能可达到如下程度：

![single_core_perf](/img/in-post/dpdk/single_core_perf.png)


*注：源代码[传送门](http://www.dpdk.org/browse/dpdk/)。*

## 2. 环境抽象层 （Environment Abstraction Layer， EAL）

## 2. 核心组件

![Core Components Architecture](/img/in-post/dpdk/architecture-overview.svg)

### 2.1 Ring Manager (librte_ring)

提供一个指定大小的无锁队列，保证多生产者、多消费者模式下数据安全。

### 2.2 Memory Pool Manager (librte_mempool)

负责从系统中申请内存并管理。

### 2.3. Network Packet Buffer Management (librte_mbuf)

用于管理缓冲区的创建和销毁，包括控制/通用消息缓冲区（ctrlmbuf）和数据包缓冲区（pktmbuf）。

### 2.4. Timer Manager (librte_timer)

为 DPDK 执行单元提供计时服务，实现异步调用。


DPDK 主要提供以下4个库：
1. 多核架构（multicore framework）
2. 内存巨页（huge page memory）
3. 环形缓冲区（ring buffers）
4. PMD驱动（poll-mode drivers for networking , crypto and eventdev）

## 2. 多核架构

DPDK 在启动时，先分析系统的 CPU 属性，根据系统默认状态生成一一绑定的映射表（用户可以根据需求修改）。

每个 CPU 属性如下：
```
class core{
    lcore_id;           //逻辑核 id
    core_id;            //物理核 id
    socket_id;          //NUMA节点 id
}

class core coremap[]     //所有逻辑核的映射表
```

在启动服务器的时候，DPDK 选取一个逻辑 CPU 做为主核，接着启动其它 CPU 做为从核，所有线程都根据映射表做 CPU 绑定。控制核主要完成 pci、内存和日志等系统的初始化；从核启动后，等待主核初始化完毕后挂载业务处理入口， 然后运行业务代码。

流程示意图如下：

![EAL Initialization in a Linux Application Environment](/img/in-post/dpdk/linuxapp_launch.svg)

*注：对象（如 memory zones, rings, memory pools, lpm tables 和 hash tables）初始化应该作整个应用初始化的一部分在主核完成。对象的创建和初始化过程并不是线程安全的，当然，一旦初始化完成，就可以在多个线程上安全并发了。*

## 3. 巨页

## 4. 环形缓冲区

区别于传统的无限大小的链式结构，DPDK 的队列管理模块 rte_ring 具有以下特征：
* 大小固定，指针存储在表中
* 无锁
* 支持单/多消费者出队列
* 支持单/多生产者入队列
* 支持按块出入队列，一次操作指定个数的对象（个数不足时，支持直接 fail 和操作最大可用数目两种模式）

相较链式结构而言，具有以下优势：
1. 速度快
    仅需要一次 sizeof(void *) 的 Compare-And-Swap 操作，比多次 double-Compare-And-Swap 具有明显优势。only requires a single Compare-And-Swap instruction of sizeof(void *) instead of several double-Compare-And-Swap instructions.
2. 比一个[完整无锁队列](https://lwn.net/Articles/340400/)实现简单
3. 可以按块初入队列
    因为指针都是存在表里的，按块出队列比链式结构有更少的 cache misses，与单个对象出队列的花费相当。

当然也有这些劣势：
1. 大小固定
2. 在内存方面环形比链式结构有更多花费
    空环也至少包含 N 个指针。

每个环形缓冲区都有一个独一无二的名称。若以相同名称创建第二个 ring， rte\_ring\_create() 接口将直接返回 NULL。

DPDK 的内存池管理内部就使用了 ring。ring 还可以用于 DPDK 应用间的通信。

![Ring Structure](/img/in-post/dpdk/ring1.svg)

上图是一个简单 ring 示例。可以发现，每个 ring 有两对指针：一对 prod\_head/prod\_tail 提供给 producers，一对  cons\_head/cons\_tail 提供给 consumers。

## 5. PMD

传统 Linux 网络协议处理过程：
```
硬件中断 --> 取包分发至内核线程 --> 软件中断 --> 内核线程在协议栈中处理包 --> 处理完毕通知用户层 --> 用户层收包 --> 网络层 --> 逻辑层 --> 业务层
```

首先网卡通过中断方式通知协议栈对数据包进行处理。协议栈先对数据包的合法性进行必要的校验，然后判断数据包目标是否为本机，满足条件则会将数据包拷贝一份向上递交给用户层来处理。不仅处理路径冗长，还需要从内核到应用层进行一次拷贝。

DPDK 对该过程做了特殊处理：
```
硬件中断 --> 放弃中断流程 --> 用户层通过设备映射取包 --> 进入用户层协议栈 --> 逻辑层 --> 业务层
```

DPDK 针对 Intel 网卡实现了基于轮询方式的 PMD（Poll Mode Drivers）驱动。该驱动由 API、用户空间运行的驱动程序构成。除了链路状态通知以外，该驱动使用无中断方式直接操作网卡的接收和发送队列。目前 PMD 驱动支持 Intel 的大部分1G、10G和40G的网卡。PMD 驱动从网卡上接收到数据包后，会直接通过 DMA 方式传输到预分配的内存中，同时更新无锁环形队列中的数据包指针，不断轮询的应用程序很快就能感知收到数据包，并在预分配的内存地址上直接处理。

概括而言有以下三个优化，在网络密集型场景下有更好表现：
1. 拦截中断，不触发后续中断流程，并绕过协议栈

    通过 UIO 重设内核中断回调行为，绕过协议的后续的处理流程。

2. 无拷贝收发包，减少内存拷贝开销

    DPDK的包全部在用户控件中使用内存池管理，内核控件与用户空间的内存交互不用进行拷贝，只做控制权转移。

3. 用户自定义协议栈，可以降低复杂度

## 6. 参考
1. [openstack_neutron_dpdk](https://chenghuiyu.gitbooks.io/openstack_neutron_dpdk/doc/3-dpdk/dpdk-tech.html)
1. [面码酱《DPDK分析》](http://www.jianshu.com/p/0ff8cb4deaef%20%20)



