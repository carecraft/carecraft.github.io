---
layout:     post
title:      "认识 DPDK"
subtitle:   "understand the theory of DPDK"
category :  basictheory
date:       2017-07-01
author:     "Max"
header-img: "img/post-bg-sync.jpg"
catalog:    true
tags:
    - opensource
---

## 1. 简介

[DPDK](http://www.dpdk.org/) 全称 Data Plane Development Kit，是 Intel 推出的高速数据包处理套件，号称可以数据包处理性能提高10倍。在英特尔 Xeon 系列 E5-2658 v4 处理器上测试，64字节数据包网络层纯转发可以达到 233 Gbps（347 Mpps）的性能。

项目上选用 dpdk 收包，因此对其原理进行分析。

*注：源代码[传送门](http://www.dpdk.org/browse/dpdk/)。*

DPDK 主要提供以下4个组件：
1. 多核架构（multicore framework）
2. 内存巨页（huge page memory）
3. 环形缓冲区（ring buffers）
4. PMD驱动（poll-mode drivers for networking , crypto and eventdev）

## 2. 多核架构

DPDK 在启动时，先分析系统的 CPU 属性，根据系统默认状态生成一一绑定的映射表（用户可以根据需求修改）。

每个 CPU 属性如下：
```
class core{
    lcore_id;           //逻辑核 id
    core_id;            //物理核 id
    socket_id;          //NUMA节点 id
}

class core coremap[ ]     //所有逻辑核的映射表
···

在启动服务器的时候，DPDK选取一个逻辑 CPU 做为主核，接着启动其它 CPU 做为从核，所有线程都根据映射表做 CPU 绑定。控制核主要完成 pci、内存和日志等系统的初始化；从核启动后，等待主核初始化完毕后挂载业务处理入口， 然后运行业务代码。

流程示意图如下：
![multCore](/img/in-post/dpdk/multCore.png)

## 2. PMD 驱动

传统 Linux 网络协议处理过程：
```
硬件中断 --> 取包分发至内核线程 --> 软件中断 --> 内核线程在协议栈中处理包 --> 处理完毕通知用户层 --> 用户层收包 --> 网络层 --> 逻辑层 --> 业务层
```

首先网卡通过中断方式通知协议栈对数据包进行处理。协议栈先对数据包的合法性进行必要的校验，然后判断数据包目标是否为本机，满足条件则会将数据包拷贝一份向上递交给用户层来处理。不仅处理路径冗长，还需要从内核到应用层进行一次拷贝。

DPDK 对该过程做了特殊处理：
```
硬件中断 --> 放弃中断流程 --> 用户层通过设备映射取包 --> 进入用户层协议栈 --> 逻辑层 --> 业务层
```

DPDK 针对 Intel 网卡实现了基于轮询方式的 PMD（Poll Mode Drivers）驱动。该驱动由 API、用户空间运行的驱动程序构成。除了链路状态通知以外，该驱动使用无中断方式直接操作网卡的接收和发送队列。目前 PMD 驱动支持 Intel 的大部分1G、10G和40G的网卡。PMD 驱动从网卡上接收到数据包后，会直接通过 DMA 方式传输到预分配的内存中，同时更新无锁环形队列中的数据包指针，不断轮询的应用程序很快就能感知收到数据包，并在预分配的内存地址上直接处理。

概括而言有以下三个优化：
1. 拦截中断，不触发后续中断流程，并绕过协议栈

    通过 UIO 重设内核中断回调行为，绕过协议的后续的处理流程。

2. 无拷贝收发包，减少内存拷贝开销

    DPDK的包全部在用户控件中使用内存池管理，内核控件与用户空间的内存交互不用进行拷贝，只做控制权转移。

3. 用户自定义协议栈，可以降低复杂度

## 参考
1. [openstack_neutron_dpdk](https://chenghuiyu.gitbooks.io/openstack_neutron_dpdk/doc/3-dpdk/dpdk-tech.html)
1. [面码酱《DPDK分析》](http://www.jianshu.com/p/0ff8cb4deaef%20%20)



